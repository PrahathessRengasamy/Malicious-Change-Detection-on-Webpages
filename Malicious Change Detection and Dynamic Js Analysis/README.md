# Alexa JS & HTML Analysis
Alexa 1M Web &amp; Search Engine Crawling for Data Discovery

## Dependencies

#### Languages

1. [Python 2.7](https://www.python.org/)
2. [GO](https://golang.org/doc/install)

#### Packages

1. NodeJS: [Download & Install](https://nodejs.org/en/download/current/)
2. [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/): `pip install beautifulsoup4`
3. [Esprima](http://esprima.org/): `npm install esprima`
4. [Scikit](http://scikit-learn.org/stable/): `pip install -U scikit-learn`
5. [coop](https://github.com/rakyll/coop): `go get github.com/rakyll/coop`
6. [boltdb](https://github.com/boltdb/bolt): `go get github.com/boltdb/bolt`
7. [progress-bar](https://github.com/cheggaaa/pb): `go get github.com/cheggaaa/pb`
8. [go-command-line-tool](https://github.com/codegangsta/cli): `go get github.com/codegangsta/cli`
9. [malware-jail](https://github.com/HynekPetrak/malware-jail)
10. [numpy](http://www.numpy.org/)
11. [matplotlib](http://matplotlib.org/users/pyplot_tutorial.html)
## Modules

Our project consists of mainly three modules, crawler, parsing and ml. We describe each module below and steps to run each module, to successfully test the program, we need to sequencially run the modules, as some of the modules depend on the data generated by the previous module. 
#### Crawler
Our crawler is written in go. The crawler reads the alexa-1m website list and crawls the data and saves them in boltdb database. Using the cronjob we run the crawler daily and collect the data. Steps to run the crawler are given below.
1. Load the sites to crawl from the .csv.gz file:
   	```
   	cd crawler/go/src/goalexa/
	go run main.go load.go goalexa.go cache
   	```
2. Start crawling, by defualt the crawler uses 100 parallel jobs but you can specify using -j JOBS (upto 256):
   	```
  	go run main.go load.go goalexa.go start -j 100
   	```
#### Data Collection
Our collected data is stored in the VM machines provided to us, you can access them in the URIs given below.
```
acs-6.cc.gatech.edu:/nethome/nposani3/db_dump/
acs-6.cc.gatech.edu:/nethome/nposani3/db_in_days/
```

#### Parser
Once we have the .db file from the output of crawler module, go program in parser that converts the .db to json. 
1. Convert .db to json file, output file name should day[NUM] eg: day10
   	``` 
   	go run parser/dbconvertor.go CRAWLED_FILE.db day[NUM] 
   	```
#### Machine Learning Classifier (ML)
We feed the json file as input to the ML module and get the scores of the websites.
1. Get dynamic features
   	``` 
   	python get_dyna.py day[NUM].json 
	mv features.txt day[NUM].txt
   	```
2. Get scores of the websites, inorder to accomplish this we need to first move the .json file to current folder of ml i.e.: day[NUM]:
   ``` 
   python all.py 
   ```
